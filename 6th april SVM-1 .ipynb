{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1de5b6d-3fa2-404e-8761-291fd8faf445",
   "metadata": {},
   "source": [
    "Certainly! Let's address each question and then proceed with the bonus task:\n",
    "\n",
    "### Q1. Mathematical Formula for a Linear SVM:\n",
    "The mathematical formula for a linear SVM decision function is given as:\n",
    "\n",
    "\\[ f(x) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b) \\]\n",
    "\n",
    "Here, \\(\\mathbf{w}\\) is the weight vector, \\(\\mathbf{x}\\) is the input vector, \\(b\\) is the bias term, and \\(\\text{sign}(\\cdot)\\) is the sign function.\n",
    "\n",
    "### Q2. Objective Function of a Linear SVM:\n",
    "The objective function for a linear SVM is to maximize the margin, which is equivalent to minimizing the norm of the weight vector (\\(\\|\\mathbf{w}\\|\\)) under the constraint that each data point is correctly classified:\n",
    "\n",
    "\\[ \\text{minimize} \\left( \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\right) \\text{ subject to } y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\text{ for all } i \\]\n",
    "\n",
    "### Q3. Kernel Trick in SVM:\n",
    "The kernel trick allows SVMs to handle non-linear decision boundaries by implicitly mapping the input features into a higher-dimensional space. The kernel function (\\(K(\\mathbf{x}_i, \\mathbf{x}_j)\\)) computes the dot product in this higher-dimensional space without explicitly calculating the transformation.\n",
    "\n",
    "### Q4. Role of Support Vectors:\n",
    "Support vectors are the data points that lie closest to the decision boundary (margin). They play a crucial role in defining the decision boundary. In case of linear SVM, the decision boundary is determined by a subset of support vectors, and these vectors have non-zero coefficients in the optimal solution.\n",
    "\n",
    "### Q5. Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM:\n",
    "- **Hyperplane:** The decision boundary that separates different classes.\n",
    "- **Marginal Plane:** Planes parallel to the hyperplane but touching the support vectors.\n",
    "- **Soft Margin:** Allows for some misclassification to achieve a wider margin. It introduces a penalty for misclassified points.\n",
    "- **Hard Margin:** Requires strict classification without any misclassification. Suitable for well-separated data.\n",
    "\n",
    "### Q6. SVM Implementation through Iris Dataset:\n",
    "Let's move on to the bonus task:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for simplicity\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot the decision boundaries\n",
    "def plot_decision_boundary(X, y, model, ax):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_decision_boundary(X_train, y_train, svm_classifier, plt.subplot(1, 2, 1))\n",
    "plt.title(\"Decision Boundaries (Training Set)\")\n",
    "\n",
    "plot_decision_boundary(X_test, y_test, svm_classifier, plt.subplot(1, 2, 2))\n",
    "plt.title(\"Decision Boundaries (Testing Set)\")\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This code trains a linear SVM classifier on the Iris dataset, predicts labels for the testing set, computes accuracy, and plots the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd15e7-6ac1-4934-a352-470efa0cabf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
